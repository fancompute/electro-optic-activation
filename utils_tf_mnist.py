import numpy as np

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Lambda
from tensorflow.python.keras import backend as K

import sys
sys.path.append("../../../neurophox")

from neurophox.keras import VCRD
from neurophox.helpers import np_to_k_complex, tf_to_k_complex, k_to_tf_complex, k_complex_from_real, k_real, k_imag

## <Helper functions for probing the activation functions within a network>

def nonlinear_layers(model):
    return [i for i in range(1,len(model.layers)-2,2)]


def get_layer_values(model):
    g_trained     = [model.layers[i].get_weights()[0][0] for i in nonlinear_layers(model)]
    phi_b_trained = [model.layers[i].get_weights()[1][0] for i in nonlinear_layers(model)]
    alpha_trained = [model.layers[i].get_weights()[2][0] for i in nonlinear_layers(model)]
    return (g_trained, phi_b_trained, alpha_trained)


def probe_trained_activations(model, N):
    inds = nonlinear_layers(model)
    X = np.matlib.repmat(np.linspace(0, 3.0, 199), N, 1).astype(np.complex64)
    X_k = np_to_k_complex(X.T)

    FX = np.zeros((len(inds), 199))
    i = 0;
    for ind in inds:
        FX_k = model.layers[ind](X_k).eval(session=K.get_session())
        tmp = k_real(FX_k) + 1j * k_imag(FX_k)
        FX[i,:] = tmp[:,0]
        i += 1
    return (X[0,:], FX.T)

## </ Helper functions for probing the activation functions within a network>


def AbsSquared(x):
    return k_complex_from_real(tf.square(k_real(x)) + tf.square(k_imag(x)))


class EOIntensityModulation(tf.keras.layers.Layer):
    def __init__(self,
                 N,
                 alpha=0.1,
                 g=np.pi,
                 phi_b=np.pi,
                 P_sat=np.inf,
                 train_alpha=False,
                 train_g=False,
                 train_phi_b=False,
                 single_param_per_layer=True,
                 bound_g=[1e-3, np.pi*1.5],
                 bound_alpha=[0.01,0.99],
                 bound_phi_b=[-np.pi,+np.pi]):
        '''
        The electro-optic intensity modulation activation function implemented for TF/Keras
        
        train_alpha, train_g, and train_phi_b specify whether each parameter should be trained. If false,
        they are fixed at the supplied values. If true, they are initialized to the supplied values and
        trained.
        
        P_sat allows the voltage signal generated by the amplifier to be saturated. This is infinite by default
        
        bound parameters specify constraints on the parameters during training
        
        single_param_per_layer specifies whether a single value for g, alpha, and phi_b should be used per
        layer. If false, then the parameters are trained element-wise (N different values for alpha, g, and 
        phi_b for each layer).    
        
        The parameters are constrained as follows:
        
        alpha:  between  0.0   and  1.0
        g:      between  1e-3  and  1.5*pi 
        phi_b:  between  -pi   and  +pi
        
        '''
        
        
        super(EOIntensityModulation, self).__init__()
        
        self.P_sat = P_sat

        if single_param_per_layer:
            var_shape = [1]
        else:
            var_shape = [N]
        
        self.g     = self.add_variable(shape=var_shape,
                                       name="g",
                                       initializer=tf.constant_initializer(g),
                                       trainable=train_g,
                                       constraint=lambda x: tf.clip_by_value(x, bound_g[0], bound_g[1]))
        self.phi_b = self.add_variable(shape=var_shape,
                                       name="phi_b",
                                       initializer=tf.constant_initializer(phi_b),
                                       trainable=train_phi_b,
                                       constraint=lambda x: tf.clip_by_value(x, bound_phi_b[0], bound_phi_b[1]))
        self.alpha = self.add_variable(shape=var_shape,
                                       name="alpha",
                                       initializer=tf.constant_initializer(alpha),
                                       trainable=train_alpha,
                                       constraint=lambda x: tf.clip_by_value(x, bound_alpha[0], bound_alpha[1]))
    
    def call(self, inputs):
        alpha, g, phi_b = tf.complex(self.alpha, 0.0), tf.complex(self.g, 0.0), tf.complex(self.phi_b, 0.0)
        Z = k_to_tf_complex(inputs)
        V = tf.complex(tf.clip_by_value(tf.real(g*tf.conj(Z)*Z), 0.0, self.P_sat), 0.0)
        fZ = 1j * tf.sqrt(1-alpha) * tf.exp(-1j*0.5*V - 1j*0.5*phi_b) * tf.cos(0.5*V + 0.5*phi_b) * Z        
        return tf_to_k_complex(fZ)
    
    def compute_output_shape(self, input_shape):
        return input_shape


def construct_onn_linear_tf(N, 
                            N_classes=10, 
                            L=1, 
                            theta_initializer='haar', 
                            phi_initializer='random_phi'):
    '''
    Constructs an L layer linear ONN model
    
    '''
    layers=[]
    
    for i in range(0,L):
        layers.append(VCRD(N, theta_initializer=theta_initializer, phi_initializer=theta_initializer))
    
    layers.append(Activation(AbsSquared))
    layers.append(Lambda(lambda x: x[:, 0, :N_classes]))
    
    return tf.keras.models.Sequential(layers)

    
def construct_onn_EO_tf(N,
                        N_classes=10,
                        L=1,
                        train_alpha=False,
                        train_g=False,
                        train_phi_b=False,
                        single_param_per_layer=True,
                        theta_initializer='haar',
                        phi_initializer='random_phi',
                        alpha=0.1,
                        g=0.1*np.pi,
                        phi_b=0*np.pi,
                        P_sat=np.inf,
                        bound_g=[1e-3, np.pi*1.5],
                        bound_alpha=[0.01,0.99],
                        bound_phi_b=[-np.pi,+np.pi],
                        attenuators=False):
    '''
    Constructs an L layer EO ONN model with the specified alpha, g, and phi_b
    
    The parameters alpha, g, and phi_b can be scalars or vectors. If scalar-valued, the value is used
    for all L layers. If vector-valued, the length must be equal to L and each vector element is used 
    for the corresponding layer.
    
    See the definition of EOIntensityModulation(tf.keras.layers.Layer) for more information on some of
    the activation function parameters
    
    attenuators specifies whether the linear layers should be arbitrary complex rather than unitary.
        If true then each linear layer is [Clements -> Attenuator bank -> Clements]
        If false then each linear layer is [Clements]
    The attenuator bank is implemented using the EO activation with g = phi_b = 0 and trainable alphas
    '''
    alpha = np.asarray(alpha)
    g     = np.asarray(g)
    phi_b = np.asarray(phi_b)
    
    if alpha.size == 1:
        alpha = np.tile(alpha, L)
    else:
        assert alpha.size == L, 'alpha has a size which is inconsistent with L'
    
    if g.size == 1:
        g = np.tile(g, L)
    else:
        assert g.size == L, 'g has a size which is inconsistent with L'
    
    if phi_b.size == 1:
        phi_b = np.tile(phi_b, L)
    else:
        assert phi_b.size == L, 'phi_b has a size which is inconsistent with L'
    
    layers=[]
    for i in range(L):
        if attenuators:
            layers.append(VCRD(N))
            # The below configuration of the EO activation essentially acts as a trainable attenuator bank
            layers.append(EOIntensityModulation(N, 0.5, 0.0, 0.0, train_alpha=True, train_g=False, train_phi_b=False, single_param_per_layer=False))
            layers.append(VCRD(N))
        else:
            layers.append(VCRD(N, theta_initializer=theta_initializer, phi_initializer=theta_initializer))
        
        layers.append(EOIntensityModulation(N,
                                            alpha[i],
                                            g[i],
                                            phi_b[i],
                                            train_alpha=train_alpha,
                                            train_g=train_g,
                                            train_phi_b=train_phi_b,
                                            single_param_per_layer=single_param_per_layer,
                                            P_sat=P_sat,
                                            bound_g=bound_g,
                                            bound_alpha=bound_alpha,
                                            bound_phi_b=bound_phi_b))
    
    layers.append(Activation(AbsSquared))
    layers.append(Lambda(lambda x: x[:, 0, :N_classes]))
    
    return tf.keras.models.Sequential(layers)

## Other helper functions

def value_to_one_hot(val, N):
    one_hot = np.zeros((N,))
    one_hot[int(val)] = 1
    return one_hot


def argmax_to_class(outputs):
    N_examples = outputs.shape[1]
    out_class = np.zeros((N_examples,))
    for i in range(N_examples):
        out_class[i] = np.argmax(outputs[:, i]) 
    return out_class


def norm_inputs(inputs, feature_axis=1):
    if feature_axis == 1:
        n_features, n_examples = inputs.shape
    elif feature_axis == 0:
        n_examples, n_features = inputs.shape
    for i in range(n_features):
        l1_norm = np.mean(np.abs(inputs[i, :]))
        inputs[i, :] /= l1_norm
    return inputs


def calc_confusion_matrix_tf(model, x_test_norm, y_test_onehot):
    y_truth = y_test_onehot.argmax(axis=-1)
    y_pred  = model.predict(np_to_k_complex(x_test_norm)).argmax(axis=-1)
    cf_matrix = tf.confusion_matrix(y_truth, y_pred).eval(session=K.get_session())
    cf_matrix = cf_matrix.astype('float') / cf_matrix.sum(axis=0)[:, np.newaxis]
    return cf_matrix
